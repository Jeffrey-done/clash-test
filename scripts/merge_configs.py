#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Clashé…ç½®åˆå¹¶è„šæœ¬ - GitHub Actionsç‰ˆæœ¬
ä¸“ä¸ºGitHub Pages + Actionsè®¾è®¡çš„ç²¾ç®€ç‰ˆï¼Œä¸éœ€è¦UIå’Œäº¤äº’
"""

import os
import sys
import yaml
import json
import logging
import asyncio
import time
import requests
from datetime import datetime
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# å¸¸é‡å®šä¹‰
CONFIGS_DIR = 'docs/configs'
CONFIG_URLS_FILE = 'config_urls.txt'
DEFAULT_TIMEOUT = 30  # è¯·æ±‚è¶…æ—¶æ—¶é—´ (ç§’)
MAX_CONCURRENT_REQUESTS = 10  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(CONFIGS_DIR, exist_ok=True)

# ä½¿ç”¨çš„ä»£ç†é…ç½®ï¼ˆé»˜è®¤ä¸ä½¿ç”¨ï¼‰
proxies = None

# ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
if os.environ.get('USE_PROXY', 'false').lower() == 'true':
    proxy_address = os.environ.get('PROXY_ADDRESS', '')
    if proxy_address:
        proxies = {
            'http': proxy_address,
            'https': proxy_address
        }
        logger.info(f"ä½¿ç”¨ä»£ç†: {proxy_address}")

async def fetch_config(url):
    """å¼‚æ­¥è·å–é…ç½®æ–‡ä»¶"""
    try:
        # ä½¿ç”¨requestsè€Œä¸æ˜¯aiohttpï¼Œå› ä¸ºGitHub Actionsç¯å¢ƒæ›´å…¼å®¹
        # å¯¹äºå°‘é‡è¯·æ±‚ï¼ŒåŒæ­¥è¯·æ±‚å½±å“ä¸å¤§
        response = requests.get(url, timeout=DEFAULT_TIMEOUT, proxies=proxies)
        response.raise_for_status()
        
        # å°è¯•è§£æYAML
        try:
            config = yaml.safe_load(response.text)
            if not isinstance(config, dict) or 'proxies' not in config:
                logger.warning(f"æ— æ•ˆçš„Clashé…ç½®: {url}")
                return None
            return config
        except Exception as e:
            logger.warning(f"è§£æé…ç½®å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
            return None
            
    except Exception as e:
        logger.warning(f"è·å–é…ç½®å¤±è´¥: {url}, é”™è¯¯: {str(e)}")
        return None

def load_urls_from_file():
    """ä»æ–‡ä»¶åŠ è½½URLåˆ—è¡¨"""
    urls = []
    try:
        if os.path.exists(CONFIG_URLS_FILE):
            with open(CONFIG_URLS_FILE, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        urls.append(line)
            logger.info(f"ä»æ–‡ä»¶åŠ è½½äº† {len(urls)} ä¸ªURL")
    except Exception as e:
        logger.error(f"åŠ è½½URLæ–‡ä»¶å¤±è´¥: {str(e)}")
    return urls

def merge_proxies(configs):
    """åˆå¹¶ä»£ç†èŠ‚ç‚¹å¹¶å»é‡"""
    if not configs:
        return [], None
        
    all_proxies = []
    first_config = None
    
    # è·å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆé…ç½®ä½œä¸ºæ¨¡æ¿
    for config in configs:
        if config and isinstance(config, dict):
            first_config = config.copy()
            break
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆé…ç½®ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿
    if not first_config:
        first_config = {
            'port': 7890,
            'socks-port': 7891,
            'allow-lan': True,
            'mode': 'rule',
            'log-level': 'info',
            'proxies': [],
            'proxy-groups': [
                {
                    'name': 'ğŸš€ èŠ‚ç‚¹é€‰æ‹©',
                    'type': 'select',
                    'proxies': ['DIRECT']
                }
            ],
            'rules': ['MATCH,DIRECT']
        }
    
    # æ”¶é›†æ‰€æœ‰ä»£ç†
    for config in configs:
        if not config or not isinstance(config, dict):
            continue
            
        proxies = config.get('proxies', [])
        if not proxies or not isinstance(proxies, list):
            continue
            
        all_proxies.extend(proxies)
    
    logger.info(f"æ”¶é›†åˆ° {len(all_proxies)} ä¸ªä»£ç†èŠ‚ç‚¹")
    
    # å»é‡ä»£ç†
    unique_proxies = remove_duplicate_proxies(all_proxies)
    logger.info(f"å»é‡åå‰©ä½™ {len(unique_proxies)} ä¸ªä»£ç†èŠ‚ç‚¹")
    
    return unique_proxies, first_config

def remove_duplicate_proxies(proxies):
    """å»é™¤é‡å¤çš„ä»£ç†èŠ‚ç‚¹"""
    unique_dict = {}
    
    for proxy in proxies:
        if not isinstance(proxy, dict):
            continue
            
        # è·³è¿‡æ— æ•ˆä»£ç†
        if 'name' not in proxy or 'type' not in proxy or 'server' not in proxy or 'port' not in proxy:
            continue
            
        # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
        proxy_copy = proxy.copy()
        proxy_copy.pop('name', None)  # åç§°å¯èƒ½ä¼šè¢«ä¿®æ”¹ï¼Œä½†èŠ‚ç‚¹æœ¬èº«å¯èƒ½ç›¸åŒ
        
        try:
            proxy_str = json.dumps(proxy_copy, sort_keys=True)
            proxy_hash = hashlib.md5(proxy_str.encode()).hexdigest()
            unique_dict[proxy_hash] = proxy
        except:
            continue
    
    return list(unique_dict.values())

def generate_config(proxies, template, filename):
    """ç”ŸæˆClashé…ç½®æ–‡ä»¶"""
    if not proxies:
        logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„ä»£ç†èŠ‚ç‚¹ï¼Œæ— æ³•ç”Ÿæˆé…ç½®")
        return False
        
    # å¤åˆ¶æ¨¡æ¿é…ç½®
    new_config = template.copy()
    
    # æ›´æ–°ä»£ç†åˆ—è¡¨
    new_config['proxies'] = proxies
    
    # æ›´æ–°ä»£ç†ç»„
    if 'proxy-groups' in new_config:
        proxy_names = [p.get('name') for p in proxies]
        
        for group in new_config['proxy-groups']:
            # å¦‚æœæ˜¯é€‰æ‹©ç±»å‹çš„ä»£ç†ç»„ï¼Œæ›´æ–°ä»£ç†åˆ—è¡¨
            if group.get('type') in ['select', 'url-test', 'fallback', 'load-balance']:
                # æ·»åŠ æ‰€æœ‰ä»£ç†å’Œå¿…è¦çš„é€‰é¡¹
                group['proxies'] = proxy_names + ['DIRECT']
    
    # æ·»åŠ å…ƒæ•°æ®
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    if 'metadata' not in new_config:
        new_config['metadata'] = {}
        
    new_config['metadata'].update({
        'updated': timestamp,
        'proxy_count': len(proxies),
        'generated_by': 'GitHub Actions'
    })
    
    # è¾“å‡ºåˆ°æ–‡ä»¶
    output_path = os.path.join(CONFIGS_DIR, filename)
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(new_config, f, default_flow_style=False, allow_unicode=True)
        
        # ä¿å­˜èŠ‚ç‚¹æ•°é‡åˆ°æ–‡ä»¶ï¼Œä¾›å‰ç«¯ä½¿ç”¨
        with open(os.path.join(CONFIGS_DIR, 'node_count.txt'), 'w') as f:
            f.write(str(len(proxies)))
            
        logger.info(f"é…ç½®å·²ä¿å­˜åˆ°: {output_path}")
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜é…ç½®å¤±è´¥: {str(e)}")
        return False

async def main():
    """ä¸»å‡½æ•°"""
    start_time = time.time()
    logger.info("å¼€å§‹å¤„ç†é…ç½®åˆå¹¶")
    
    # åŠ è½½URL
    urls = load_urls_from_file()
    if not urls:
        logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é…ç½®URL")
        return 1
    
    # è·å–é…ç½®
    configs = []
    for url in urls:
        config = await fetch_config(url)
        if config:
            configs.append(config)
    
    if not configs:
        logger.error("æ²¡æœ‰è·å–åˆ°æœ‰æ•ˆçš„é…ç½®")
        return 1
    
    # åˆå¹¶å’Œå»é‡ä»£ç†
    proxies, template = merge_proxies(configs)
    if not proxies:
        logger.error("æ²¡æœ‰æœ‰æ•ˆçš„ä»£ç†èŠ‚ç‚¹")
        return 1
    
    # ç”Ÿæˆé…ç½®æ–‡ä»¶
    output_filename = f"clash_config_{datetime.now().strftime('%Y%m%d')}.yaml"
    if generate_config(proxies, template, output_filename):
        # ç”Ÿæˆæœ€æ–°ç‰ˆæœ¬çš„å‰¯æœ¬
        generate_config(proxies, template, "latest.yaml")
    
    elapsed = time.time() - start_time
    logger.info(f"å¤„ç†å®Œæˆï¼Œç”¨æ—¶ {elapsed:.2f} ç§’")
    return 0

if __name__ == "__main__":
    sys.exit(asyncio.run(main())) 